{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "from transformers import pipeline\n",
    "\n",
    "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
    "pipe = pipeline(\"text-classification\", model=model_ckpt, device=-1)\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_language(lyrics: str) -> str|np.nan:\n",
    "    res = pipe([lyrics], truncation=True, max_length=128)\n",
    "    return res[0]['label'] if res[0]['score'] > 0.5 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.read_csv(\n",
    "    \"../data/raw/song_lyrics.csv\",\n",
    "    chunksize=5 * 10**4,\n",
    "    usecols=[\"title\", \"artist\", \"year\", \"tag\", \"views\", \"lyrics\"],\n",
    "    dtype={\"year\": np.int16, \"views\": np.int32}\n",
    ") as chunks:\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {idx}\")\n",
    "\n",
    "        # drop N.A. lyrics\n",
    "        chunk = chunk.dropna(subset=[\"lyrics\"])\n",
    "        \n",
    "        # drop romanizations\n",
    "        chunk = chunk[chunk[\"artist\"] != \"Genius Romanizations\"]\n",
    "        chunk = chunk[~chunk[\"title\"].str.contains(r\"\\(?romanized\\)?\", regex=True, na=False, case=False)]\n",
    "\n",
    "        # remove invalid years\n",
    "        chunk = chunk[chunk[\"year\"] < 2023]\n",
    "        \n",
    "        # remove duplicated entries\n",
    "        chunk = chunk.drop_duplicates(subset=[\"title\", \"artist\", \"year\"])\n",
    "        \n",
    "        # remove special characters from lyrics\n",
    "        pattern = r\"(?m)^\\[.*?\\]$\"\n",
    "        chunk[\"lyrics\"] = chunk[\"lyrics\"].str.replace(pattern, \"\", regex=True)\n",
    "        \n",
    "        # remove empty lines\n",
    "        pattern = r\"\\n|\\n\\n\"\n",
    "        chunk[\"lyrics\"] = chunk[\"lyrics\"].str.replace(pattern, \" \", regex=True)\n",
    "\n",
    "        # drop lyrics that are too short or too long\n",
    "        chunk = chunk[chunk[\"lyrics\"].str.len().between(10**2, 10**5)]\n",
    "\n",
    "        # analyze language\n",
    "        chunk[\"language\"] = chunk[\"lyrics\"].apply(identify_language)\n",
    "        print(f'{len(chunk[chunk[\"language_cld3\"].isna()])} not identified lyrics using by the language detection model.')\n",
    "        \n",
    "        # drop non-english lyrics\n",
    "        chuck = chunk[chunk[\"language\"] == \"en\"][[\"artist\", \"tag\", \"lyrics\"]]\n",
    "        \n",
    "        # save processed data\n",
    "        chunk.to_csv(\"../data/processed/lyrics_processed.csv\", mode=\"a\", header=not os.path.exists(\"../data/processed/lyrics_processed.csv\"), index=False)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
